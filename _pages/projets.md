---
permalink: /projets/
title: "Projets"
classes: wide
sidebar: false
---

<style>
/* Forcer la largeur maximale pour le contenu */
.page__content {
    max-width: 100% !important;
    width: 100% !important;
}

/* S'assurer que le conteneur principal utilise toute la largeur */
.page {
    max-width: 100% !important;
    width: 100% !important;
}

/* Optimiser l'iframe pour la responsivit√© */
.full-width-iframe {
    width: 100%;
    height: 1200px;
    border: none;
    margin: 0;
    padding: 0;
}

.full-width-iframe-model_size {
    width: 100%;
    height: 500px;
    border: none;
    margin: 0;
    padding: 0;
}

.full-width-iframe-treemap {
    width: 100%;
    height: 800px;
    border: none;
    margin: 0;
    padding: 0;
}
  
/* Media query pour les √©crans plus petits */
@media (max-width: 768px) {
    .full-width-iframe {
        height: 400px;
    }
}

<div style="width: 100%; margin: 0; padding: 0;">
	
<p>
Sur cette page vous pouvez trouver les contenus sur lesquels j'ai travaill√© (√† titre personnel ou professionnel) mais qui sont r√©f√©renc√©s sur d'autres sites que mon blog personnel. Il s'agit principalement de traductions de cours, et des cr√©ations de jeux de donn√©es et de mod√®les.</p>

<br><br>

<!--
# Vue d'ensemble

<iframe
	src="https://lbourdois-roadmap.static.hf.space"
	frameborder="0"
	width="950"
	height="1500"
></iframe>

<br><br><br>
-->

<h1>Mod√®les et jeux de donn√©es en fran√ßais</h1>

<h2>FAT5</h2>
<p>Le FAT5 est une impl√©mentation du <a href="https://arxiv.org/abs/1910.10683">T5</a> en PyTorch avec un objectif <a href="https://arxiv.org/abs/2205.05131">UL2</a> optimis√© pour GPGPU d√©velopp√© avec <a href="https://b-albar.github.io/portfolio/">Boris ALBAR</a>.<br>
Elle utilise des noyaux CUDA et Triton personnalis√©s ainsi que des optimisations sp√©cifiques pour augmenter le d√©bit et r√©duire l'utilisation de la m√©moire pour l'entra√Ænement et l'inf√©rence d'un facteur 2 par rapport √† l'impl√©mentation originale disponible dans Hugging Face.<br>
Nous l'avons appliqu√©e en pr√©-entra√Ænant un mod√®le en fran√ßais de 147M param√®tres en utilisant uniquement une A100. Nous estimons ainsi pouvoir ramener le prix de pr√©-entra√Ænement d'un tel mod√®le √† seulement 1200‚Ç¨ (estimation faite sur une instance Sesterce).<br>
Le code de pr√©-entrainement est disponible sur <a href="https://github.com/catie-aq/flashT5">GitHub</a> sous licence Apache-2.0 et les poids du mod√®le entra√Æn√© sur le compte <a href="https://huggingface.co/collections/CATIE-AQ/catie-french-fat5-ul2-677697a35feea336389d6403">Hugging Face du CATIE</a>. Un article de blog d√©taillant notre m√©thodologie est disponible <a href="https://huggingface.co/spaces/CATIE-AQ/FAT5-rapport">ici</a>.</p>

<h2>NER</h2>
<p>Les NERmemBERT constituent une famille de mod√®les de Reconnaissance d'Entit√©s Nomm√©es en fran√ßais capable d'√©tiqueter jusqu'√† 4 entit√©s (Personnalit√©s, Lieux, Organisations, Divers tel que des noms d'≈ìuvre, de maladies, etc.). Ils sont disponibles en taille base (110M ou 136M de param√®tres) et large (336M), g√©rant des contextes allant de 512 √† 8192 tokens. Les poids sont disponibles gratuitement en open-source, tout comme les jeux de donn√©es ayant servis √† l'entra√Ænement. Le tout est disponible sur le compte <a href="https://huggingface.co/collections/CATIE-AQ/catie-french-ner-pack-658aefafe3f7a2dcf0e4dbb4">Hugging Face du CATIE</a>. Un article de blog d√©taillant la m√©thodologie adopt√©e est disponible <a href="https://lbourdois.github.io/blog/NER/">ici</a>.<br>
Ils ont √©t√© t√©l√©charg√©s plus de 185 000 fois depuis leur mise en ligne.</p>

<h2>Question Answering</h2>
<p>Les QAmemBERT constituent une famille de r√©ponse aux questions en fran√ßais capable d'indiquer si la r√©ponse √† une question est pr√©sente ou pas dans un texte de contexte associ√©. Ils sont disponibles en taille base (110M ou 136M de param√®tres) et large (335M), g√©rant des contextes allant de 512 √† 8192 tokens. Les poids sont disponibles gratuitement en open-source, tout comme le jeu de donn√©es ayant servis √† l'entra√Ænement. Le tout est disponible sur le compte <a href="https://huggingface.co/collections/CATIE-AQ/catie-french-qa-pack-650821750f44c341cdb8ec91">Hugging Face du CATIE</a>. Un article de blog d√©taillant la m√©thodologie adopt√©e est disponible <a href="https://lbourdois.github.io/blog/QA/">ici</a>.<br>
Ils ont √©t√© t√©l√©charg√©s plus de 160 000 fois depuis leur mise en ligne.</p>

<h2>DFP</h2>
<p><em>Dataset of French Prompts</em> (DFP) contient 113 129 978 lignes portant sur 30 t√¢ches de NLP diff√©rentes.<br>
724 prompts ont √©t√© √©crits sous forme imp√©rative, de tutoiement et de vouvoiement afin de couvrir autant que possible les donn√©es de pr√©-entra√Ænement utilis√©es par le mod√®le qui utilisera ces donn√©es et qui nous sont inconnues.</p>

<p>Les colonnes <em>inputs</em> et <em>targets</em> suivent le m√™me format que l'ensemble de donn√©es <a href="https://huggingface.co/datasets/bigscience/xP3">xP3</a> de Muennighoff et al.<br>
L'ensemble des d√©tails est disponible sur <a href="https://huggingface.co/datasets/CATIE-AQ/DFP">Hugging Face</a>.<br>
Il a √©t√© t√©l√©charg√© plus de 75 000 fois depuis sa mise en ligne.</p>

<h2>La marmite</h2>
<p>Projet toujours en cours.<br>
Il s'agit de proposer un √©quivalent en fran√ßais du jeu de donn√©es <a href="https://huggingface.co/datasets/HuggingFaceM4/the_cauldron"><em>the cauldron</em></a> afin de pouvoir entra√Æner un VLM en fran√ßais.<br>
Ce jeu de donn√©es prendra en compte des donn√©es d'OCR (voir celles d√©j√† disponibles en ligne <a href="https://huggingface.co/collections/lbourdois/french-ocr-datasets-67c8d3152330f11227e0d108">ici</a>), de captionning (voir celles d√©j√† disponibles en ligne <a href="https://huggingface.co/collections/lbourdois/french-caption-datasets-67c8d2227284a5daa00c50b9">ici</a>), de VQA (voir celles d√©j√† disponibles en ligne <a href="https://huggingface.co/collections/lbourdois/french-vqa-datasets-67c8d1a162a23ef0e9a2bc89">ici</a>) et de raisonnement.<br>
Les sous-jeux de donn√©es d√©j√† en ligne ont √©t√© t√©l√©charg√© plus de 25 000 fois depuis leur mise en ligne.</p>

<br><br><br>

<h1>Traductions</h1>

<h2>Cours de Yann Le Cun et Alfredo Canziani de la NYU</h2>
<p>Cette traduction a √©t√© la plus longue √† effectuer s'√©talant de 2020 √† 2022.<br>
Le contenu est structur√© en 19 unit√©s r√©parties sur 33 vid√©os üé• de cours (cours magistraux et travaux dirig√©s) d'une dur√©e totale d'environ 45H, 74 pages web üåê r√©sumant les vid√©os via les notes prises par les √©tudiants pendant le cours, et 16 notebooks Jupyter üìì (en PyTorch) utilis√©s lors des TD. Enfin un jeu de donn√©es de plus de 3000 donn√©es parall√®les v√©rifi√©es manuellement a √©t√© cr√©√© pour entra√Æner un mod√®le de traduction.<br>
Vous pouvez retrouver toutes ces ressources sur le site internet d√©di√© qui a √©t√© con√ßu √† l'occasion : <a href="https://lbourdois.github.io/cours-dl-nyu/">https://lbourdois.github.io/cours-dl-nyu/</a>.</p>

<h2>Cours de Hugging Face ü§ó</h2>

<h3>Cours de NLP</h3>
<p>En 2022, j'ai traduit le cours de traitement automatique du langage de Hugging Face.<br>
Le contenu est structur√© en 10 chapitres comprenant un total de 76 vid√©os üé• d'une dur√©e totale d'environ 5H, de 78 pages web üåê et 61 notebooks Jupyter üìì  (en PyTorch et Tensorflow).<br>
Vous pouvez retrouver toutes ces ressources sur le site de <a href="https://huggingface.co/learn/nlp-course/fr/chapter1/1">Hugging Face</a>.</p>

<h3>Cours d'audio</h3>
<p>En 2023, j'ai traduit le cours d'audio de Hugging Face.<br>
Le contenu est structur√© en 8 unit√©s r√©parties sur 46 pages web üåê.<br>
Vous pouvez retrouver toutes ces ressources sur le site de <a href="https://huggingface.co/learn/audio-course/fr/">Hugging Face</a>.</p>

<h3>Cours sur les mod√®les de diffusion</h3>
<p>En 2023, j'ai traduit le cours sur les mod√®les de diffusion de Hugging Face.<br>
Le contenu est structur√© en 4 chapitres portant sur 17 pages web üåê et 8 notebooks Jupyter üìì (en PyTorch).<br>
Vous pouvez retrouver toutes ces ressources sur le GitHub de <a href="https://github.com/huggingface/diffusion-models-class/tree/main/units/fr">Hugging Face</a> (le contenu n'ayant pas encore √©t√© propag√© sur le site officiel).</p>

<h3>Cours sur les agents IA</h3>
<p>En 2025, j'ai particip√© avec <a href="https://github.com/knoel99">Kim NOEL</a> √† la traduction du cours sur les agents IA de Hugging Face.<br>
Le contenu est structur√© en 4 unit√©s (+ 3 bonus) r√©parties sur 74 pages web üåê et 16 notebooks Jupyter üìì.<br>
Vous pouvez retrouver toutes ces ressources sur le site de <a href="https://huggingface.co/learn/agents-course/fr">Hugging Face</a>.</p>

<h3>Guide sur l'√©valuation des LLM</h3>
<p>En  2025, j'ai traduit le <a href="https://github.com/huggingface/evaluation-guidebook/tree/main">guide</a> de <a href="https://huggingface.co/clefourrier">Cl√©mentine Fourrier</a>.<br>
Le contenu est structur√© 5 chapitres r√©partis sur 30 pages web üåê et 3 notebooks Jupyter üìì.<br>
Vous pouvez retrouver toutes ces ressources <a href="https://huggingface.co/spaces/CATIE-AQ/Guide_Evaluation_LLM">ici</a>.</p>
</div>
